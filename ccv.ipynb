{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Coherence Vectors for Image Classification\n",
    "*Implemented by [Jing Yu Koh](http://kohjingyu.com).*\n",
    "\n",
    "This notebook implements the method of using color coherence vectors for image classification. For more information, refer to the [CCV paper](https://www.cs.cornell.edu/~rdz/Papers/PZM-MM96.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_adjacent(x1, y1, x2, y2):\n",
    "    ''' Returns true if (x1, y1) is adjacent to (x2, y2), and false otherwise '''\n",
    "    x_diff = abs(x1 - x2)\n",
    "    y_diff = abs(y1 - y2)\n",
    "    return not (x_diff == 1 and y_diff == 1) and (x_diff <= 1 and y_diff <= 1)\n",
    "\n",
    "def find_max_cliques(arr, n):\n",
    "    ''' Returns a 2*n dimensional vector\n",
    "    v_i, v_{i+1} describes the number of coherent and incoherent pixels respectively a given color\n",
    "    '''\n",
    "    tau = int(arr.shape[0] * arr.shape[1] * 0.01) # Classify as coherent is area is >= 1%\n",
    "    ccv = [0 for i in range(n**3 * 2)]\n",
    "    unique = np.unique(arr)\n",
    "    for u in unique:\n",
    "        x, y = np.where(arr == u)\n",
    "        groups = []\n",
    "        coherent = 0\n",
    "        incoherent = 0\n",
    "                \n",
    "        for i in range(len(x)):\n",
    "            found_group = False\n",
    "            for group in groups:\n",
    "                if found_group:\n",
    "                    break\n",
    "\n",
    "                for coord in group:\n",
    "                    xj, yj = coord\n",
    "                    if is_adjacent(x[i], y[i], xj, yj):\n",
    "                        found_group = True\n",
    "                        group[(x[i], y[i])] = 1\n",
    "                        break\n",
    "            if not found_group:\n",
    "                groups.append({(x[i], y[i]): 1})\n",
    "        \n",
    "        for group in groups:\n",
    "            num_pixels = len(group)\n",
    "            if num_pixels >= tau:\n",
    "                coherent += num_pixels\n",
    "            else:\n",
    "                incoherent += num_pixels\n",
    "        \n",
    "        assert(coherent + incoherent == len(x))\n",
    "        \n",
    "        index = int(u)\n",
    "        ccv[index*2] = coherent\n",
    "        ccv[index*2+1] = incoherent\n",
    "    \n",
    "    return ccv\n",
    "    \n",
    "def get_ccv(img, n):\n",
    "    # Blur pixel slightly using avg pooling with 3x3 kernel\n",
    "    blur_img = cv2.blur(img, (3,3))\n",
    "    blur_flat = blur_img.reshape(32*32, 3)\n",
    "    \n",
    "    # Discretize colors\n",
    "    hist, edges = np.histogramdd(blur_flat, bins=n)\n",
    "    \n",
    "    graph = np.zeros((img.shape[0], img.shape[1]))\n",
    "    result = np.zeros(blur_img.shape)\n",
    "    \n",
    "    total = 0 \n",
    "    for i in range(0, n):\n",
    "        for j in range(0, n):\n",
    "            for k in range(0, n):\n",
    "                rgb_val = [edges[0][i+1], edges[1][j+1], edges[2][k+1]]\n",
    "                previous_edge = [edges[0][i], edges[1][j], edges[2][k]]\n",
    "                coords = ((blur_img <= rgb_val) & (blur_img >= previous_edge)).all(axis=2)\n",
    "                result[coords] = rgb_val\n",
    "                graph[coords] = i + j * n + k * n**2\n",
    "    \n",
    "    result = result.astype(int)\n",
    "    return find_max_cliques(graph, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # indicating 2^3 discretized colors\n",
    "feature_size = n**3 * 2 # Number of discretized colors * 2 for coherent and incoherent\n",
    "\n",
    "def extract_features(image):\n",
    "    return get_ccv(image, n) # image.flatten()\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    p = np.random.permutation(len(data))\n",
    "    return data[p], labels[p]\n",
    "\n",
    "def load_data(dataset=\"train\", classes=[\"airplane\", \"automobile\", \"bird\", \"cat\"]):\n",
    "    random.seed(1337)\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, c in enumerate(classes):\n",
    "        for file in glob.glob(\"data/{}/{}/*.jpg\".format(dataset, c)):\n",
    "            one_hot_label = np.zeros(len(classes))\n",
    "            one_hot_label[i] = 1\n",
    "            labels.append(one_hot_label)\n",
    "            \n",
    "            img = np.array(Image.open(file))\n",
    "            features = extract_features(img)\n",
    "            data.append(features)\n",
    "    \n",
    "    data, labels = np.array(data), np.array(labels)\n",
    "    \n",
    "    if dataset == \"train\":\n",
    "        data, labels = shuffle_data(data, labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "We try out CCV on binary classification of two classes: bird and cat. Training set consists of 20 images from CIFAR-10 for each class. Testing set also contains a different set of 20 images for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.6931471824645996\n",
      "Epoch: 11, loss: 644.6712646484375\n",
      "Epoch: 21, loss: 90.36966705322266\n",
      "Epoch: 31, loss: 313.6954650878906\n",
      "Epoch: 41, loss: 250.98727416992188\n",
      "Epoch: 51, loss: 315.92041015625\n",
      "Epoch: 61, loss: 98.74835205078125\n",
      "Epoch: 71, loss: 238.23143005371094\n",
      "Epoch: 81, loss: 47.603675842285156\n",
      "Epoch: 91, loss: 114.12418365478516\n",
      "Train accuracy: 0.775\n",
      "Test accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohjingyu/Documents/School/Term6/01.112/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kohjingyu/Documents/School/Term6/01.112/venv/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "classes = [\"bird\", \"cat\"]\n",
    "num_classes = len(classes)\n",
    "\n",
    "train_data, train_labels = load_data(dataset=\"train\", classes=classes)\n",
    "test_data, test_labels = load_data(dataset=\"test\", classes=classes)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "base_lr = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, feature_size]) # Placeholder for image input\n",
    "y = tf.placeholder(tf.float32, [None, num_classes]) # Placeholder for labels\n",
    "\n",
    "# Model parameters\n",
    "# Weights and bias\n",
    "w = tf.Variable(tf.zeros([feature_size, num_classes]))\n",
    "b = tf.Variable(tf.zeros([num_classes]))\n",
    "\n",
    "pred = tf.matmul(x, w) + b\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# cost = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.multiply(tf.scalar_mul(2,y) - 1, pred), axis=1))) # Logistic loss\n",
    "# cost = tf.reduce_mean(tf.maximum(1 - tf.multiply(tf.scalar_mul(2, y) - 1, pred), 0)) # Hinge loss\n",
    "# cost = tf.reduce_mean(tf.losses.hinge_loss(y, pred))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# For early stopping\n",
    "eps = 0.0001 # If loss decreases below this amount, stop training\n",
    "last_loss = None\n",
    "losses_to_consider = 5 # If the mean of the last 3 losses < eps, stop\n",
    "losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        train_batch, train_label = train_data[batch_start:batch_end], train_labels[batch_start:batch_end]\n",
    "\n",
    "        _, batch_cost = sess.run([optimizer, cost], feed_dict={x: train_batch,\n",
    "                                                      y: train_label, lr: base_lr / (epoch+1)})\n",
    "        losses.append(batch_cost)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: {}, loss: {}\".format(epoch+1, batch_cost))\n",
    "        \n",
    "        last_losses = np.mean(losses[-1-losses_to_consider:-1])\n",
    "\n",
    "        if abs(last_losses) < eps:\n",
    "            break\n",
    "        \n",
    "        # Shuffle data again\n",
    "        train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: train_data, y: train_labels}))\n",
    "    print(\"Test accuracy:\", accuracy.eval({x: test_data, y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Using kNN\n",
    "Attempt multi-class classification using kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 16)\n"
     ]
    }
   ],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\"]\n",
    "num_classes = len(classes)\n",
    "\n",
    "train_data, train_labels = load_data(dataset=\"train\", classes=classes)\n",
    "test_data, test_labels = load_data(dataset=\"test\", classes=classes)\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "k = 2\n",
    "nn = NearestNeighbors(n_neighbors=k).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3875\n"
     ]
    }
   ],
   "source": [
    "distance, indices = nn.kneighbors(test_data)\n",
    "pred_sum = np.zeros(test_labels.shape)\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    index = indices[i]\n",
    "    for j in range(k):\n",
    "        pred_sum[i] += train_labels[index[j]]\n",
    "\n",
    "class_pred = np.argmax(pred_sum, axis=1)\n",
    "preds = np.zeros(pred_sum.shape)\n",
    "preds[range(len(class_pred)), class_pred] = 1 # Convert highest in axis to one hot\n",
    "\n",
    "num_correct = np.all(np.equal(preds, test_labels),axis=1) # Check number of rows that are equal\n",
    "accuracy = np.mean(num_correct.astype(int)) # Get accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Using Logistic Regression\n",
    "We use logistic regression here to do multi-class classification. Generally, this seems to perform better than kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.6931471824645996\n",
      "Epoch: 11, loss: 519.8790283203125\n",
      "Epoch: 21, loss: 436.6109924316406\n",
      "Epoch: 31, loss: 170.10304260253906\n",
      "Epoch: 41, loss: 189.2510986328125\n",
      "Epoch: 51, loss: 142.00942993164062\n",
      "Epoch: 61, loss: 71.0032958984375\n",
      "Epoch: 71, loss: 60.660945892333984\n",
      "Epoch: 81, loss: 47.96961212158203\n",
      "Epoch: 91, loss: 59.73826599121094\n",
      "Train accuracy: 0.45\n",
      "Test accuracy: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kohjingyu/Documents/School/Term6/01.112/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kohjingyu/Documents/School/Term6/01.112/venv/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\"]\n",
    "num_classes = len(classes)\n",
    "\n",
    "# train_data, train_labels = load_data(dataset=\"train\", classes=classes)\n",
    "# test_data, test_labels = load_data(dataset=\"test\", classes=classes)\n",
    "batch_size = 16\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "base_lr = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, feature_size]) # Placeholder for image input\n",
    "y = tf.placeholder(tf.float32, [None, num_classes]) # Placeholder for labels\n",
    "\n",
    "# Model parameters\n",
    "# Weights and bias\n",
    "w = tf.Variable(tf.zeros([feature_size, num_classes]))\n",
    "b = tf.Variable(tf.zeros([num_classes]))\n",
    "\n",
    "pred = tf.matmul(x, w) + b\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# cost = tf.reduce_mean(tf.exp(-tf.reduce_sum(tf.multiply(tf.scalar_mul(2,y) - 1, pred), axis=1))) # Logistic loss\n",
    "# cost = tf.reduce_mean(tf.maximum(1 - tf.multiply(tf.scalar_mul(2, y) - 1, pred), 0)) # Hinge loss\n",
    "# cost = tf.reduce_mean(tf.losses.hinge_loss(y, pred))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# For early stopping\n",
    "eps = 0.0001 # If loss decreases below this amount, stop training\n",
    "last_loss = None\n",
    "losses_to_consider = 3 # If the mean of the last 3 losses < eps, stop\n",
    "losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        train_batch, train_label = train_data[batch_start:batch_end], train_labels[batch_start:batch_end]\n",
    "\n",
    "        _, batch_cost = sess.run([optimizer, cost], feed_dict={x: train_batch,\n",
    "                                                      y: train_label, lr: base_lr / (epoch+1)})\n",
    "        losses.append(batch_cost)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: {}, loss: {}\".format(epoch+1, batch_cost))\n",
    "        \n",
    "        last_losses = np.mean(losses[-1-losses_to_consider:-1])\n",
    "\n",
    "        if abs(last_losses) < eps:\n",
    "            break\n",
    "        \n",
    "        # Shuffle data again\n",
    "        train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: train_data, y: train_labels}))\n",
    "    print(\"Test accuracy:\", accuracy.eval({x: test_data, y: test_labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_class",
   "language": "python",
   "name": "ml_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
